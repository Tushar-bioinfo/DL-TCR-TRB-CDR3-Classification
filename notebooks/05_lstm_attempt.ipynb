{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EMBED_DIM = 32\n",
    "HIDDEN_DIM = 64\n",
    "NUM_EPOCHS = 90\n",
    "MAX_SEQ_LEN = 22\n",
    "NUM_CDR3S = 40\n",
    "VOCAB_SIZE = 22  # 20 amino acids + <PAD>=0 + <UNK>=1\n",
    "LABEL2IDX = {'tumor': 1, 'normal': 0}\n",
    "\n",
    "# File paths\n",
    "basedir = Path(\"/Users/tusharsingh/Work/Project/DL-cdr3-tumor\")\n",
    "jsonl_path = basedir / \"processed\" / \"cdr3_tumor_normal.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDR3Dataset(Dataset):\n",
    "    def __init__(self, jsonl_path):\n",
    "        self.samples = []\n",
    "        with open(jsonl_path, 'r') as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                label = LABEL2IDX[item['label']]\n",
    "                cdr3_tensor = torch.tensor(item['cdr3s'], dtype=torch.long)\n",
    "                self.samples.append((cdr3_tensor, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CDR3Dataset(jsonl_path)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, L = x.size()  # B=batch, T=CDR3s, L=AA per CDR3\n",
    "        x = x.view(B * T, L)\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]  # Take last LSTM output\n",
    "        x = x.view(B, T, -1).permute(0, 2, 1)  # B x H x T\n",
    "        x = self.pool(x).squeeze(2)\n",
    "        x = self.fc(x)\n",
    "        return self.sigmoid(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMClassifier(vocab_size=VOCAB_SIZE, embed_dim=EMBED_DIM, hidden_dim=HIDDEN_DIM).to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lx/5td2g65s51s2qn_zywp584340000gn/T/ipykernel_4813/4162132213.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x, y = x.to(device), torch.tensor(y, dtype=torch.float32).to(device)\n",
      "/var/folders/lx/5td2g65s51s2qn_zywp584340000gn/T/ipykernel_4813/4162132213.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x, y = x.to(device), torch.tensor(y, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.8678, Acc = 0.4941\n",
      "\t\tVal Acc = 0.5234\n",
      "Epoch 2: Train Loss = 0.7115, Acc = 0.5106\n",
      "\t\tVal Acc = 0.4860\n",
      "Epoch 3: Train Loss = 0.7021, Acc = 0.4518\n",
      "\t\tVal Acc = 0.5047\n",
      "Epoch 4: Train Loss = 0.6962, Acc = 0.4682\n",
      "\t\tVal Acc = 0.4860\n",
      "Epoch 5: Train Loss = 0.6974, Acc = 0.4988\n",
      "\t\tVal Acc = 0.5140\n",
      "Epoch 6: Train Loss = 0.7029, Acc = 0.5082\n",
      "\t\tVal Acc = 0.4860\n",
      "Epoch 7: Train Loss = 0.7005, Acc = 0.4871\n",
      "\t\tVal Acc = 0.4860\n",
      "Epoch 8: Train Loss = 0.7113, Acc = 0.4988\n",
      "\t\tVal Acc = 0.5140\n",
      "Epoch 9: Train Loss = 0.7093, Acc = 0.4729\n",
      "\t\tVal Acc = 0.4860\n",
      "Epoch 10: Train Loss = 0.6980, Acc = 0.4565\n",
      "\t\tVal Acc = 0.5234\n",
      "Epoch 11: Train Loss = 0.6946, Acc = 0.4541\n",
      "\t\tVal Acc = 0.4860\n",
      "Epoch 12: Train Loss = 0.6997, Acc = 0.5106\n",
      "\t\tVal Acc = 0.5140\n",
      "Epoch 13: Train Loss = 0.7079, Acc = 0.4800\n",
      "\t\tVal Acc = 0.4860\n",
      "Epoch 14: Train Loss = 0.7110, Acc = 0.5012\n",
      "\t\tVal Acc = 0.5140\n",
      "Epoch 15: Train Loss = 0.7014, Acc = 0.4965\n",
      "\t\tVal Acc = 0.5234\n",
      "Epoch 16: Train Loss = 0.6976, Acc = 0.4941\n",
      "\t\tVal Acc = 0.4953\n",
      "Epoch 17: Train Loss = 0.6943, Acc = 0.4965\n",
      "\t\tVal Acc = 0.5234\n",
      "Epoch 18: Train Loss = 0.6935, Acc = 0.5200\n",
      "\t\tVal Acc = 0.4766\n",
      "Epoch 19: Train Loss = 0.6923, Acc = 0.5153\n",
      "\t\tVal Acc = 0.5234\n",
      "Epoch 20: Train Loss = 0.6933, Acc = 0.4941\n",
      "\t\tVal Acc = 0.5140\n",
      "Epoch 21: Train Loss = 0.6911, Acc = 0.4988\n",
      "\t\tVal Acc = 0.4766\n",
      "Epoch 22: Train Loss = 0.6953, Acc = 0.5129\n",
      "\t\tVal Acc = 0.4673\n",
      "Epoch 23: Train Loss = 0.6941, Acc = 0.5059\n",
      "\t\tVal Acc = 0.5047\n",
      "Epoch 24: Train Loss = 0.6973, Acc = 0.4894\n",
      "\t\tVal Acc = 0.4860\n",
      "Epoch 25: Train Loss = 0.6965, Acc = 0.4988\n",
      "\t\tVal Acc = 0.5234\n",
      "Epoch 26: Train Loss = 0.6978, Acc = 0.4729\n",
      "\t\tVal Acc = 0.4860\n",
      "Epoch 27: Train Loss = 0.6943, Acc = 0.4965\n",
      "\t\tVal Acc = 0.4860\n",
      "Epoch 28: Train Loss = 0.6944, Acc = 0.5129\n",
      "\t\tVal Acc = 0.4860\n",
      "Epoch 29: Train Loss = 0.6931, Acc = 0.5153\n",
      "\t\tVal Acc = 0.4860\n",
      "Epoch 30: Train Loss = 0.6948, Acc = 0.4776\n",
      "\t\tVal Acc = 0.4860\n",
      "Epoch 31: Train Loss = 0.6924, Acc = 0.5294\n",
      "\t\tVal Acc = 0.4860\n",
      "Epoch 32: Train Loss = 0.6912, Acc = 0.4871\n",
      "\t\tVal Acc = 0.4953\n",
      "Epoch 33: Train Loss = 0.6995, Acc = 0.4847\n",
      "\t\tVal Acc = 0.4953\n",
      "Epoch 34: Train Loss = 0.6949, Acc = 0.5176\n",
      "\t\tVal Acc = 0.4860\n",
      "Epoch 35: Train Loss = 0.6945, Acc = 0.4918\n",
      "\t\tVal Acc = 0.5234\n",
      "Epoch 36: Train Loss = 0.6933, Acc = 0.5153\n",
      "\t\tVal Acc = 0.4860\n",
      "Epoch 37: Train Loss = 0.6935, Acc = 0.4800\n",
      "\t\tVal Acc = 0.4860\n",
      "Epoch 38: Train Loss = 0.6942, Acc = 0.5176\n",
      "\t\tVal Acc = 0.4673\n",
      "Epoch 39: Train Loss = 0.6950, Acc = 0.4894\n",
      "\t\tVal Acc = 0.4860\n",
      "Epoch 40: Train Loss = 0.6935, Acc = 0.5153\n",
      "\t\tVal Acc = 0.4766\n",
      "Epoch 41: Train Loss = 0.6914, Acc = 0.5176\n",
      "\t\tVal Acc = 0.4673\n",
      "Epoch 42: Train Loss = 0.6918, Acc = 0.5176\n",
      "\t\tVal Acc = 0.5794\n",
      "Epoch 43: Train Loss = 0.6893, Acc = 0.5059\n",
      "\t\tVal Acc = 0.5234\n",
      "Epoch 44: Train Loss = 0.6911, Acc = 0.5106\n",
      "\t\tVal Acc = 0.5047\n",
      "Epoch 45: Train Loss = 0.6928, Acc = 0.5247\n",
      "\t\tVal Acc = 0.4673\n",
      "Epoch 46: Train Loss = 0.6892, Acc = 0.5529\n",
      "\t\tVal Acc = 0.5327\n",
      "Epoch 47: Train Loss = 0.6950, Acc = 0.4965\n",
      "\t\tVal Acc = 0.4860\n",
      "Epoch 48: Train Loss = 0.6856, Acc = 0.5388\n",
      "\t\tVal Acc = 0.5701\n",
      "Epoch 49: Train Loss = 0.6850, Acc = 0.5624\n",
      "\t\tVal Acc = 0.4953\n",
      "Epoch 50: Train Loss = 0.6889, Acc = 0.5506\n",
      "\t\tVal Acc = 0.5421\n",
      "Epoch 51: Train Loss = 0.7045, Acc = 0.4682\n",
      "\t\tVal Acc = 0.5140\n",
      "Epoch 52: Train Loss = 0.7002, Acc = 0.5082\n",
      "\t\tVal Acc = 0.4860\n",
      "Epoch 53: Train Loss = 0.6968, Acc = 0.5082\n",
      "\t\tVal Acc = 0.5140\n",
      "Epoch 54: Train Loss = 0.6957, Acc = 0.4753\n",
      "\t\tVal Acc = 0.4766\n",
      "Epoch 55: Train Loss = 0.6915, Acc = 0.5129\n",
      "\t\tVal Acc = 0.5047\n",
      "Epoch 56: Train Loss = 0.6924, Acc = 0.5200\n",
      "\t\tVal Acc = 0.5140\n",
      "Epoch 57: Train Loss = 0.6922, Acc = 0.5012\n",
      "\t\tVal Acc = 0.5047\n",
      "Epoch 58: Train Loss = 0.6909, Acc = 0.5106\n",
      "\t\tVal Acc = 0.4673\n",
      "Epoch 59: Train Loss = 0.6887, Acc = 0.5106\n",
      "\t\tVal Acc = 0.4673\n",
      "Epoch 60: Train Loss = 0.6889, Acc = 0.5294\n",
      "\t\tVal Acc = 0.5607\n",
      "Epoch 61: Train Loss = 0.6944, Acc = 0.5176\n",
      "\t\tVal Acc = 0.5234\n",
      "Epoch 62: Train Loss = 0.6939, Acc = 0.5247\n",
      "\t\tVal Acc = 0.5327\n",
      "Epoch 63: Train Loss = 0.6939, Acc = 0.5059\n",
      "\t\tVal Acc = 0.5047\n",
      "Epoch 64: Train Loss = 0.6901, Acc = 0.5106\n",
      "\t\tVal Acc = 0.5234\n",
      "Epoch 65: Train Loss = 0.6987, Acc = 0.5035\n",
      "\t\tVal Acc = 0.4953\n",
      "Epoch 66: Train Loss = 0.6917, Acc = 0.5129\n",
      "\t\tVal Acc = 0.5421\n",
      "Epoch 67: Train Loss = 0.6892, Acc = 0.5224\n",
      "\t\tVal Acc = 0.5607\n",
      "Epoch 68: Train Loss = 0.6909, Acc = 0.5482\n",
      "\t\tVal Acc = 0.5234\n",
      "Epoch 69: Train Loss = 0.6972, Acc = 0.5035\n",
      "\t\tVal Acc = 0.4579\n",
      "Epoch 70: Train Loss = 0.6991, Acc = 0.5412\n",
      "\t\tVal Acc = 0.5701\n",
      "Epoch 71: Train Loss = 0.6833, Acc = 0.5482\n",
      "\t\tVal Acc = 0.5421\n",
      "Epoch 72: Train Loss = 0.6755, Acc = 0.5929\n",
      "\t\tVal Acc = 0.5140\n",
      "Epoch 73: Train Loss = 0.6716, Acc = 0.5765\n",
      "\t\tVal Acc = 0.5514\n",
      "Epoch 74: Train Loss = 0.6698, Acc = 0.5906\n",
      "\t\tVal Acc = 0.5327\n",
      "Epoch 75: Train Loss = 0.6727, Acc = 0.5859\n",
      "\t\tVal Acc = 0.5327\n",
      "Epoch 76: Train Loss = 0.6694, Acc = 0.6000\n",
      "\t\tVal Acc = 0.5421\n",
      "Epoch 77: Train Loss = 0.6688, Acc = 0.5929\n",
      "\t\tVal Acc = 0.5421\n",
      "Epoch 78: Train Loss = 0.6721, Acc = 0.5624\n",
      "\t\tVal Acc = 0.4299\n",
      "Epoch 79: Train Loss = 0.6748, Acc = 0.5647\n",
      "\t\tVal Acc = 0.4766\n",
      "Epoch 80: Train Loss = 0.7045, Acc = 0.5365\n",
      "\t\tVal Acc = 0.5514\n",
      "Epoch 81: Train Loss = 0.6870, Acc = 0.5576\n",
      "\t\tVal Acc = 0.5607\n",
      "Epoch 82: Train Loss = 0.6866, Acc = 0.5106\n",
      "\t\tVal Acc = 0.5514\n",
      "Epoch 83: Train Loss = 0.6846, Acc = 0.5082\n",
      "\t\tVal Acc = 0.4953\n",
      "Epoch 84: Train Loss = 0.6918, Acc = 0.5271\n",
      "\t\tVal Acc = 0.5047\n",
      "Epoch 85: Train Loss = 0.6792, Acc = 0.5294\n",
      "\t\tVal Acc = 0.5327\n",
      "Epoch 86: Train Loss = 0.6846, Acc = 0.5647\n",
      "\t\tVal Acc = 0.5701\n",
      "Epoch 87: Train Loss = 0.6764, Acc = 0.5671\n",
      "\t\tVal Acc = 0.5047\n",
      "Epoch 88: Train Loss = 0.6882, Acc = 0.5176\n",
      "\t\tVal Acc = 0.4953\n",
      "Epoch 89: Train Loss = 0.6741, Acc = 0.5647\n",
      "\t\tVal Acc = 0.5607\n",
      "Epoch 90: Train Loss = 0.6732, Acc = 0.5741\n",
      "\t\tVal Acc = 0.5327\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0, 0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), torch.tensor(y, dtype=torch.float32).to(device)\n",
    "        preds = model(x)\n",
    "        loss = loss_fn(preds, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * len(y)\n",
    "        correct += ((preds > 0.5).long() == y.long()).sum().item()\n",
    "        total += len(y)\n",
    "\n",
    "    train_acc = correct / total\n",
    "\n",
    "    model.eval()\n",
    "    val_correct, val_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), torch.tensor(y, dtype=torch.float32).to(device)\n",
    "            preds = model(x)\n",
    "            val_correct += ((preds > 0.5).long() == y.long()).sum().item()\n",
    "            val_total += len(y)\n",
    "\n",
    "    val_acc = val_correct / val_total\n",
    "    print(f\"Epoch {epoch}: Train Loss = {train_loss/total:.4f}, Acc = {train_acc:.4f}\")\n",
    "    print(f\"\\t\\tVal Acc = {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We experimented with an LSTM architecture to better capture sequential dependencies in each CDR3 sequence. However, the LSTM model also plateaued early in training:\n",
    "\n",
    "Best Validation Accuracy: ~56.0%\n",
    "\n",
    "Training Accuracy (Epoch 90): 57.4%\n",
    "\n",
    "Val Accuracy (Epoch 90): 53.2%\n",
    "\n",
    "Despite being more expressive, the LSTM model failed to significantly outperform mean pooling.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
